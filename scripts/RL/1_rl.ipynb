{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete, Box, Sequence, Dict\n",
    "import numpy as np\n",
    "from utils import methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CONFIG = {\n",
    "    'alpha_range': range(2, 8),\n",
    "    'beta_range': [round(i * 0.5, 1) for i in range(2, 9)],\n",
    "    'h_range': [round(i * 0.01, 2) for i in range(6, 61)],\n",
    "    'c_range': range(20, 30),\n",
    "    'total': range(10, 40), \n",
    "}\n",
    "\n",
    "def get_realized_data(config):\n",
    "    alpha = np.random.choice(config['alpha_range'])\n",
    "    beta = np.random.choice(config['beta_range'])\n",
    "    h = np.random.choice(config['h_range'])\n",
    "    c = np.random.choice(config['c_range'])\n",
    "    total = np.random.choice(config['total'])\n",
    "    intervals = np.random.gamma(shape=alpha, scale=beta, size=total)\n",
    "    travel_time = sum(intervals[3:]) - np.random.exponential(scale=beta)\n",
    "    travel_time = max(beta * 2, travel_time)\n",
    "\n",
    "    return alpha, beta, h, c, total, intervals, travel_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, step_size=1):\n",
    "        super(CustomEnv, self).__init__()\n",
    "\n",
    "        self.alpha = -1\n",
    "        self.beta = -1\n",
    "        self.h = -1\n",
    "        self.c = -1\n",
    "        self.total = -1\n",
    "        self.intervals = -1\n",
    "        self.travel_time = -1\n",
    "        self.cur_time = -1\n",
    "        self.obs_intervals = -1\n",
    "        self.n = -1\n",
    "        self.N = -1\n",
    "        self.cum_sum_intervals = -1\n",
    "\n",
    "        self.step_size = step_size\n",
    "\n",
    "        # 0 = wait, 1 = leave\n",
    "        self.action_space = Discrete(2)\n",
    "\n",
    "        self.observation_space = Dict({\n",
    "            'h': Box(0, max(CONFIG['h_range']) + 1),\n",
    "            'c': Box(0, max(CONFIG['c_range']) + 1),\n",
    "            'n': Discrete(max(CONFIG['total']) + 1),\n",
    "            'N': Discrete(max(CONFIG['total']) + 1),\n",
    "            'obs_intervals': Sequence(Box(0, np.inf)),\n",
    "            'travel_time': Box(0, np.inf),\n",
    "            'cur_time': Box(0, np.inf)\n",
    "        })\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        return {\n",
    "            'h': self.h,\n",
    "            'c': self.c,\n",
    "            'n': self.n,\n",
    "            'N': self.N,\n",
    "            'obs_intervals': self.obs_intervals,\n",
    "            'travel_time': self.travel_time,\n",
    "            'cur_time': self.cur_time\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        self.alpha, self.beta, self.h, self.c, self.total, self.intervals, self.travel_time = get_realized_data(CONFIG)\n",
    "        \n",
    "        self.cum_sum_intervals = np.cumsum(self.intervals)\n",
    "\n",
    "        self.obs_intervals = self.intervals[:3]\n",
    "        self.n =  3\n",
    "        self.N = self.total - self.n\n",
    "        self.cur_time = self.cum_sum_intervals[self.n - 1]\n",
    "\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        self.cur_time += self.step_size\n",
    "\n",
    "        if self.cur_time >= self.cum_sum_intervals[-1]:\n",
    "            action = 1\n",
    "\n",
    "        if action == 0:\n",
    "            while self.cur_time >= self.cum_sum_intervals[self.n]:\n",
    "                self.n += 1\n",
    "                self.N -= 1\n",
    "                self.obs_intervals.append(self.intervals[self.n])\n",
    "            return self._get_obs(), 0, False, {}\n",
    "        else:\n",
    "            cost = methods.cal_cost(c=self.c, h=self.h, actual_time=self.cum_sum_intervals[-1], predicted_time=self.cur_time + self.travel_time)\n",
    "            self.obs_intervals = self.intervals[:self.total]\n",
    "            self.n = self.total\n",
    "            self.N = 0\n",
    "            return self._get_obs(), -cost, True, {} \n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(self._get_obs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = CustomEnv()\n",
    "\n",
    "# experiment with the environment\n",
    "c.reset()\n",
    "done = False \n",
    "while not done:\n",
    "    _, reward, done, _ = c.step(np.random.choice([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]))\n",
    "    print(c._get_obs())\n",
    "    print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_baselines3\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "\n",
    "# Create the environment\n",
    "env = CustomEnv()\n",
    "env.reset()\n",
    "\n",
    "# Instantiate the age nt\n",
    "# model = PPO(\"MultiInputPolicy\", env, verbose=1)\n",
    "model = PPO(\"MultiInputPolicy\", env, device=\"cpu\")\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=100)\n",
    "\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_custom_env\")\n",
    "\n",
    "# Load the model\n",
    "model = PPO.load(\"ppo_custom_env\")\n",
    "\n",
    "# Test the trained agent\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    env.render(mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlClass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
